{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e684cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast,AutoTokenizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4839f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Full_Data_With_Perturbed_Text_1000Prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cdcece9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Text</th>\n",
       "      <th>LabelName</th>\n",
       "      <th>Label</th>\n",
       "      <th>Perturbed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
       "      <td>Basically there are many categories of \" Best ...</td>\n",
       "      <td>Human Answer</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there are many categories of \" Best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
       "      <td>If you 're hearing about it , it 's because it...</td>\n",
       "      <td>Human Answer</td>\n",
       "      <td>0</td>\n",
       "      <td>If you 're hearing about it , it 's because it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
       "      <td>One reason is lots of catagories . However , h...</td>\n",
       "      <td>Human Answer</td>\n",
       "      <td>0</td>\n",
       "      <td>One is, and already is, lots of good books to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
       "      <td>There are many different best seller lists tha...</td>\n",
       "      <td>ChatGPT Answer</td>\n",
       "      <td>1</td>\n",
       "      <td>There are many different best seller lists tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If salt is so bad for cars , why do we use it ...</td>\n",
       "      <td>salt is good for not dying in car crashes and ...</td>\n",
       "      <td>Human Answer</td>\n",
       "      <td>0</td>\n",
       "      <td>salt is good for not dying in car crashes and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Why is every book I hear about a \" NY Times # ...   \n",
       "1  Why is every book I hear about a \" NY Times # ...   \n",
       "2  Why is every book I hear about a \" NY Times # ...   \n",
       "3  Why is every book I hear about a \" NY Times # ...   \n",
       "4  If salt is so bad for cars , why do we use it ...   \n",
       "\n",
       "                                                Text       LabelName  Label  \\\n",
       "0  Basically there are many categories of \" Best ...    Human Answer      0   \n",
       "1  If you 're hearing about it , it 's because it...    Human Answer      0   \n",
       "2  One reason is lots of catagories . However , h...    Human Answer      0   \n",
       "3  There are many different best seller lists tha...  ChatGPT Answer      1   \n",
       "4  salt is good for not dying in car crashes and ...    Human Answer      0   \n",
       "\n",
       "                                      Perturbed Text  \n",
       "0  Basically there are many categories of \" Best ...  \n",
       "1  If you 're hearing about it , it 's because it...  \n",
       "2  One is, and already is, lots of good books to ...  \n",
       "3  There are many different best seller lists tha...  \n",
       "4  salt is good for not dying in car crashes and ...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bc71bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e28402d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Perturbed Text'].notna()] #Removing Sequence with more than 512 and failed masked attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73835b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3876"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7ab01d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced56ac7",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8082e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data for original text\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(df['Text'], df['Label'], \n",
    "                                                                    random_state=0, \n",
    "                                                                    test_size=0.4, \n",
    "                                                                    stratify=df['Label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f342c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data for perturbed text\n",
    "\n",
    "train_text_p, test_text_p, train_labels_p, test_labels_p = train_test_split(df['Perturbed Text'], df['Label'], \n",
    "                                                                    random_state=0, \n",
    "                                                                    test_size=0.4, \n",
    "                                                                    stratify=df['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e1ba9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 9459, 3189, 2005, 5958, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT tokenizer testing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.batch_encode_plus([\"Thesis report for friday\"], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bd944b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mural\\Envs\\ML\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#selecting sequence length, padding to meet the length of 512, and truncate the large ones\n",
    "# Original Text\n",
    "\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 400,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 400,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6906840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting sequence length, padding to meet the length of 512, and truncate the large ones\n",
    "# Perturbed Text\n",
    "\n",
    "tokens_train_p = tokenizer.batch_encode_plus(\n",
    "    train_text_p.tolist(),\n",
    "    max_length = 400,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens_test_p = tokenizer.batch_encode_plus(\n",
    "    test_text_p.tolist(),\n",
    "    max_length = 400,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc7a8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the input tokens to tensors\n",
    "#original text\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab005942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the input tokens to tensors\n",
    "#perturbed text\n",
    "\n",
    "train_seq_p = torch.tensor(tokens_train_p['input_ids'])\n",
    "train_mask_p = torch.tensor(tokens_train_p['attention_mask'])\n",
    "train_y_p = torch.tensor(train_labels_p.tolist())\n",
    "\n",
    "test_seq_p = torch.tensor(tokens_test_p['input_ids'])\n",
    "test_mask_p = torch.tensor(tokens_test_p['attention_mask'])\n",
    "test_y_p = torch.tensor(test_labels_p.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0758b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2325, 400])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.size() # Input Tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e275e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2325, 400])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq_p.size() # Input Tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a01ae1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for Original Text\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 1\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "76fb32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for Perturbed Text\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 1\n",
    "\n",
    "# wrap tensors\n",
    "train_data_p = TensorDataset(train_seq_p, train_mask_p, train_y_p)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler_p = SequentialSampler(train_data_p)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader_p = DataLoader(train_data_p, sampler=train_sampler_p, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2424a",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fee85",
   "metadata": {},
   "source": [
    "### BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9bba524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#importing bert and freezing and the layers initial\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# for param in bert.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5145f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model architecture\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "#         # dropout layer\n",
    "        \n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "#         # relu activation function\n",
    "        \n",
    "#         self.relu =  nn.ReLU()\n",
    "        \n",
    "#         # dense layer 1\n",
    "        \n",
    "#         self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "#         # dense layer 2 (Output layer)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(512,2)\n",
    "        \n",
    "#         #softmax activation function\n",
    "        \n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
    "        \n",
    "        x = cls_hs\n",
    "        \n",
    "#         x = self.fc1(cls_hs)\n",
    "        \n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         # output layer\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         # apply softmax activation\n",
    "        \n",
    "#         x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "756e78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c207a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mural\\Envs\\ML\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d4f93038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define loss function\n",
    "\n",
    "cross_entropy  = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8037f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "actual = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b70ae3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Based Zero Shot Learning\n",
    "\n",
    "model.train()\n",
    "    \n",
    "for ori_itr, pert_itr in zip(train_dataloader, train_dataloader_p):\n",
    "    \n",
    "    sample_ori = [r.to(device) for r in ori_itr]\n",
    "    sample_pert = [r.to(device) for r in pert_itr]\n",
    "    \n",
    "    sent_id, mask, labels = sample_ori\n",
    "    sent_id_p, mask_p, labels_p = sample_pert\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds_ori = model(sent_id, mask)\n",
    "        preds_pert = model(sent_id_p, mask_p)\n",
    "    \n",
    "#     print(preds_pert.size())\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    output = cos(preds_ori, preds_pert)\n",
    "    \n",
    "    \n",
    "    \n",
    "    actual.append(labels.item())\n",
    "    predictions.append(output)\n",
    "    \n",
    "    preds_ori.detach()\n",
    "    preds_pert.detach()\n",
    "    \n",
    "#     print(labels.item())\n",
    "\n",
    "    \n",
    "    # display the output tensor\n",
    "#     print(\"Cosine Similarity:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2212e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = []\n",
    "for i in predictions:\n",
    "    predictions2.append(i.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b0f4f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- Results--------------------------\n",
      "AUROC Score: 0.6083526570048309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mural\\Envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py:1141: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y)\n",
      "C:\\Users\\mural\\Envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py:1141: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.asarray(y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "actual = np.array(actual)\n",
    "# predictions = np.array(predictions)\n",
    "fpr, tpr, _ = roc_curve(actual, predictions2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"--------------------------------- Results--------------------------\")\n",
    "print(\"AUROC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48571ce",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0eee70",
   "metadata": {},
   "source": [
    "#### GPT Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "74902e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gpt = []\n",
    "actual_gpt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a29026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0becc2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6750b697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,    11,   616,  3290,   318, 13779]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bc922691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "50df90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c4cec6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0][5].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b78f8",
   "metadata": {},
   "source": [
    "-------------------- Experiment -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ad315499",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_gpt = []\n",
    "predictions_gpt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "63fd6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Based Zero Shot Learning\n",
    "\n",
    "model.train()\n",
    "\n",
    "for ori_itr, pert_itr, label in zip(train_text, train_text_p, train_labels):\n",
    "    \n",
    "    inputs_ori = tokenizer(ori_itr, return_tensors=\"pt\")\n",
    "    outputs_ori = model(**inputs_ori)\n",
    "    \n",
    "    inputs_pert = tokenizer(pert_itr, return_tensors=\"pt\")\n",
    "    outputs_pert = model(**inputs_pert)\n",
    "    \n",
    "    n1 = inputs_ori['input_ids'].size()[1]\n",
    "    n2 = inputs_pert['input_ids'].size()[1]\n",
    "    \n",
    "    vec_ori = outputs_ori[0][0][n1-1]\n",
    "    vec_pert = outputs_pert[0][0][n2-1]\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    output = cos(preds_ori, preds_pert)\n",
    "    \n",
    "    actual_gpt.append(label)\n",
    "    predictions_gpt.append(output.item())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "44fc5c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [172], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m predictions2 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m predictions_gpt:\n\u001b[1;32m----> 3\u001b[0m     predictions2\u001b[38;5;241m.\u001b[39mappend(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "predictions2 = []\n",
    "for i in predictions_gpt:\n",
    "    predictions2.append(i.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "501a8b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- Results--------------------------\n",
      "AUROC Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "actual2 = np.array(actual_gpt)\n",
    "predictions2 = np.array(predictions_gpt)\n",
    "fpr, tpr, _ = roc_curve(actual2, predictions2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"--------------------------------- Results--------------------------\")\n",
    "print(\"AUROC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
